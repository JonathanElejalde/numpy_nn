{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rnn.ipynb","provenance":[],"authorship_tag":"ABX9TyOeLcTDYGcgsunXqhDT1fiY"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"akmGHfMF2eVI","executionInfo":{"status":"ok","timestamp":1603580857635,"user_tz":300,"elapsed":632,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7HFpa2oxKuDy"},"source":["# RNN model"]},{"cell_type":"markdown","metadata":{"id":"rlOipu0d511N"},"source":["### Utility functions"]},{"cell_type":"code","metadata":{"id":"DdvXruMN52nS","executionInfo":{"status":"ok","timestamp":1603586643177,"user_tz":300,"elapsed":511,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def softmax(x):\n","  e_x = np.exp(x - np.max(x))\n","  \n","  return e_x / e_x.sum(axis=0)\n","\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"MouXgSwn3fgN","executionInfo":{"status":"ok","timestamp":1603581646290,"user_tz":300,"elapsed":714,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def rnn_cell_forward(timestep_t, a_prev, parameters):\n","  \"\"\"\n","  Implements a single forward step of an RNN cell\n","\n","  Args:\n","    timestep_t: (numpy array) input data at timestep \"t\"\n","    a_prev: (numpy array) hidden state at timestep \"t-1\"\n","    parameters: (dict) containing:\n","      Wax: (numpy array) weight matrix multiplying the input\n","      Waa: (numpy array) weight matrix multiplying the hidden state\n","      Wya: (numpy array) weight matrix relating the hiden state\n","        to the output\n","      ba: (numpy array) bias\n","      by: (numpy array) bias relating the hidden state to the output\n","\n","  Returns:\n","    a_next: (numpy array) next hidden state\n","    yt_pred: (numpy array) prediction at timestep \"t\"\n","    cache: (tuple) with values needed for the backward pass\n","      it contains (a_nex, a_prev, timestep_t, parameters)\n","  \"\"\"\n","\n","  Wax = parameters['Wax']\n","  Waa = parameters['Waa']\n","  Wya = parameters['Wya']\n","  ba = parameters['ba']\n","  by = parameters['by']\n","\n","  # Compute next activation state\n","  a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, timestep_t) + ba)\n","\n","  # Compute output of the current cell\n","  yt_pred = softmax(np.dot(Wya, a_next) + by)\n","\n","  cache = (a_next, a_prev, timestep_t, parameters)\n","\n","  return a_next, yt_pred, cache"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lMKyzSdE6PUd"},"source":["# RNN forward pass"]},{"cell_type":"code","metadata":{"id":"-lAX4hG37kl4","executionInfo":{"status":"ok","timestamp":1603582489993,"user_tz":300,"elapsed":510,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def rnn_forward(x, act0, parameters):\n","  \"\"\"\n","  Implements the forward propagation of a recurrent neural\n","  network\n","\n","  Args:\n","    x: (numpy array) input data for every timestep\n","    act0: (numpy array) initial hidden state\n","    parameters: (dict) containing:\n","      Wax: (numpy array) weight matrix multiplying the input\n","      Waa: (numpy array) weight matrix multiplying the hidden state\n","      Wya: (numpy array) weight matrix relating the hiden state\n","        to the output\n","      ba: (numpy array) bias\n","      by: (numpy array) bias relating the hidden state to the output\n","\n","  Returns:\n","    activations: (numpy array) hidden states for every timestep\n","    y_pred: (numpy array) predictions for every timestep\n","    caches: (tuple) with values needed for the backward pass\n","      it contains the list of caches and x\n","  \"\"\"\n","\n","  caches = []\n","\n","  # get dimensions from shapes of x and Wy\n","  n_x, m, T_x = x.shape\n","  n_y, n_a = parameters['Wya'].shape\n","\n","  # initialize 'a' and 'y' with zeros\n","  activations = np.zeros((n_a, m, T_x))\n","  y_pred = np.zeros((n_y, m, T_x))\n","\n","  # initialize a_next\n","  a_next = act0\n","\n","  for timestep in range(T_x):\n","    # update next hidden state, compute the prediction\n","    a_next, yt_pred, cache = rnn_cell_forward(x[:, :, timestep], a_next, parameters)\n","\n","    # save the value of the new 'next' hidden state and the prediction\n","    activations[:, :, timestep] = a_next\n","    y_pred[:, :, timestep] = yt_pred\n","    caches.append(cache)\n","\n","  caches = (caches, x)\n","\n","  return activations, y_pred, caches"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Rtp-ctb9qQY"},"source":["# Backprop in a RNN\n"]},{"cell_type":"code","metadata":{"id":"7x0m0U6iABuj","executionInfo":{"status":"ok","timestamp":1603583723196,"user_tz":300,"elapsed":519,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def rnn_cell_backward(da_next, cache):\n","  \"\"\"\n","  Implements the backward pass for a single step in the RNN-cell\n","\n","  Args:\n","    da_next: (numpy array) gradient loss with respect to next hidden state\n","    cache: (dict) contains outputs of the rnn_cell_forward()\n","\n","  Returns:\n","    gradients: (dict) containing:\n","      dx: (numpy array)\n","      da_prev: (numpy array) gradients of previous hidden state\n","      dWax: (numpy array) gradients of input to hidden weights\n","      dWaa: (numpy array) gradients of hidden to hidden weights\n","      dba: (numpy array) gradients of bias vector\n","  \"\"\"\n","\n","  a_next, a_prev, timestep_t, parameters = cache\n","\n","  Wax = parameters['Wax']\n","  Waa = parameters['Waa']\n","  Wya = parameters['Wya']\n","  ba = parameters['ba']\n","  by = parameters['by']\n","\n","  # compute the gradient of tanh with respect to a_next\n","  dtanh = (1 - a_next**2) * da_next\n","\n","  # compute the gradient of the loss with respect to Wax\n","  dxt = np.dot(Wax.T, dtanh)\n","  dWax = np.dot(dtanh, timestep_t.T)\n","\n","  # Compute the gradient with respect to Waa\n","  da_prev = np.dot(Waa.T, dtanh)\n","  dWaa = np.dot(dtanh, a_prev.T)\n","\n","  # compute the gradient with respect to b\n","  dba = np.sum(dtanh, 1, keepdims=True)\n","\n","  # store gradients\n","  gradients = {'dx': dxt,\n","               'dWax': dWax,\n","               'da_prev': da_prev,\n","               'dWaa': dWaa,\n","               'dba': dba}\n","\n","  return gradients"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Jtts6p1Cezz","executionInfo":{"status":"ok","timestamp":1603585779888,"user_tz":300,"elapsed":618,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def rnn_backward(da, caches):\n","  \"\"\"\n","  Implements the backward pass for a RNN over an entire sequence\n","  of input data\n","\n","  Args:\n","    da: upstream gradient of all hidden states\n","    caches: (tuple) containing information from the forward pass\n","\n","  Returns:\n","    gradients: (dict) containing:\n","      dx: gradients of the input data\n","      da0: gradients of the initial hidden state\n","      dWax: gradients of the input's weights matrix\n","      dWaa: gradients of the hidden state's weights matrix\n","      dba: gradients of the bias\n","  \"\"\"\n","\n","  caches, x = caches\n","  a1, a0, x1, parameters = caches[0]\n","\n","  # get dimensions\n","  n_a, m, T_x = da.shape\n","  n_x, m = x1.shape\n","\n","  # initialize the gradients with the right sizes\n","  dx = np.zeros((n_x, m, T_x))\n","  dWax = np.zeros((n_a, n_x))\n","  dWaa = np.zeros((n_a, n_a))\n","  dba = np.zeros((n_a, 1))\n","  da0 = np.zeros((n_a, m))\n","  da_prevt = np.zeros((n_a, m))\n","\n","  for timestep in reversed(range(T_x)):\n","    # compute gradients in current timestep\n","    gradients = rnn_cell_backward(da[:, :, timestep] + da_prevt, caches[timestep])\n","\n","    # get derivatives from gradients\n","    dxt = gradients['dxt'] \n","    da_prevt = gradients['da_prev']\n","    dWaxt = gradients['dWax']\n","    dWaat = gradients['dWaa']\n","    dbat = gradients['dba']\n","\n","    # increment the global derivatives\n","    dx[:, :, timestep] = dxt\n","    dWax += dWaxt\n","    dWaa += dWaat\n","    dba += dbat\n","\n","  # set da0 to the gradient of \"a\" which has been backpropagated thorugh\n","  # all the timesteps\n","  da0 = da_prevt\n","\n","  gradients = {'dx': dx,\n","               'da0': da0,\n","               'dWax': dWax,\n","               'dWaa': dWaa,\n","               'dba': dba}\n","\n","  return gradients  \n"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vm-af3S2KUyl"},"source":["# LSTM model\n"]},{"cell_type":"code","metadata":{"id":"slODx2JHK1wB","executionInfo":{"status":"ok","timestamp":1603590247515,"user_tz":300,"elapsed":519,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def lstm_cell_forward(timestep_t, a_prev, c_prev, parameters):\n","  \"\"\"\n","  Implements a single forward step of a LSTM cell\n","\n","  Args:\n","    timestep_t: (numpy array) input data at timestep 't'\n","    a_prev: (numpy array) hidden state at timestep 't-1'\n","    c_prev: (numpy array) memory state at timestep 't-1'\n","    parameters: (dict) containing:\n","      Wf: weight matrix of the forget gate\n","      bf: bias of the forget gate\n","      Wu: weight matrix of the update gate\n","      bu: bias of the update gate\n","      Wc: weight matrix of the first \"tanh\"\n","      bc: bias of the first \"tanh\"\n","      Wo: weight matrix of the output gate\n","      bo: bias of the output gate\n","      Wy: weight matrix relating the hidden state to the output\n","      by: bias relating the hidden state to the output\n","\n","  Returns: \n","    a_next: (numpy array) next hidden state\n","    c_next: (numpy array) next memory state\n","    yt_pred: (numpy array) prediction at timestep 't'\n","    cache: (tuple) with values needed for the backprop\n","  \"\"\"\n","  Wf = parameters[\"Wf\"]\n","  bf = parameters[\"bf\"]\n","  Wu = parameters[\"Wu\"]\n","  bu = parameters[\"bu\"]\n","  Wc = parameters[\"Wc\"]\n","  bc = parameters[\"bc\"]\n","  Wo = parameters[\"Wo\"]\n","  bo = parameters[\"bo\"]\n","  Wy = parameters[\"Wy\"]\n","  by = parameters[\"by\"]\n","\n","  # get rigth dimensions\n","  n_x, m = timestep_t.shape\n","  n_y, n_a = Wy.shape\n","\n","  # concatenate a_prev and timestep_t\n","  concat = np.zeros((n_a + n_x, m))\n","  concat[:n_a, :] = a_prev\n","  concat[n_a:, :] = timestep_t\n","\n","  # compute gates\n","  for_t = sigmoid(np.dot(Wf, concat) + bf)\n","  up_t = sigmoid(np.dot(Wu, concat) + bu)\n","  cct = np.tanh(np.dot(Wc, concat) + bc)\n","  c_next = for_t * c_prev + up_t * cct\n","  out_t = sigmoid(np.dot(Wo, concat) + bo)\n","  a_next = out_t * np.tanh(c_next)\n","\n","  # compute prediction of the LSTM cell\n","  yt_pred = softmax(np.dot(Wy, a_next) + by)\n","\n","  cache = (a_next, c_next, a_prev, c_prev, for_t, up_t, cct, out_t, timestep_t, parameters)\n","\n","  return a_next, c_next, yt_pred, cache\n"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"fkQ4ugnDOfFm","executionInfo":{"status":"ok","timestamp":1603590248326,"user_tz":300,"elapsed":375,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def lstm_forward(x, a0, parameters):\n","  \"\"\"\n","  Implements the forward pass of the RNN using an LSTM cell\n","\n","  Args:\n","    x: input data for every timestep\n","    a0: initial hidden state\n","    parameters: (dict) containing:\n","      Wf: weight matrix of the forget gate\n","      bf: bias of the forget gate\n","      Wu: weight matrix of the update gate\n","      bu: bias of the update gate\n","      Wc: weight matrix of the first \"tanh\"\n","      bc: bias of the first \"tanh\"\n","      Wo: weight matrix of the output gate\n","      bo: bias of the output gate\n","      Wy: weight matrix relating the hidden state to the output\n","      by: bias relating the hidden state to the output\n","\n","  Returns:\n","    activations: (numpy array) hidden states for every timestep\n","    y: (numpy array) predictions for every timestep\n","    c: (numpy array) cell state for every timestep\n","    caches: tuple with values needed for the backprop\n","  \"\"\"\n","  caches = []\n","\n","  # get right dimensions\n","  n_x, m, T_x = x.shape\n","  n_y, n_a = parameters['Wy'].shape\n","\n","  # initialize\n","  activations = np.zeros((n_a, m, T_x))\n","  c = activations\n","  y = np.zeros((n_y, m, T_x))\n","  a_next = a0\n","  c_next = np.zeros(a_next.shape)\n","\n","  for timestep in range(T_x):\n","    # update next hidden state, next meomry state, and compute predictions\n","    a_next, c_next, yt, cache = lstm_cell_forward(x[:, :, timestep], a_next, c_next, parameters)\n","\n","    # save the value of the new \"next\" hidden state, the prediction\n","    # and the next cell state\n","    activations[:, :, timestep] = a_next\n","    y[:, :, timestep] = yt\n","    c[:, :, timestep] = c_next\n","    caches.append(cache)\n","\n","  caches = (caches, x)\n","\n","  return activations, y, c, caches"],"execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A9UXaoLnQtq5"},"source":["# Backpropagation for LSTM"]},{"cell_type":"code","metadata":{"id":"W4_22y5CRAAH","executionInfo":{"status":"ok","timestamp":1603590287680,"user_tz":300,"elapsed":595,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def lstm_cell_backward(da_next, dc_next, cache):\n","  \"\"\"\n","  Implements a single timestep backprop for the lstm cell\n","\n","  Args:\n","    da_next: gradients of next hidden state\n","    dc_next: gradients of next cell state\n","    cache: information from the forward pass\n","\n","  Returns:\n","    gradients: (dict) containing:\n","      dxt: gradient of input data at timestep t\n","      da_prev: gradient of the previous hidden state\n","      dc_prev: gradient  of the previous memory state\n","      dWf: gradient of the weight matrix of the forget gate\n","      dWu: gradient of the weight matrix of the update gate\n","      dWc: gradient of the weight matrix of the memory gate\n","      dWo: gradient of the weight matrix of the output gate\n","      dbf: gradient of the baises of the fortget gate\n","      dbu: gradient of the biases of the update gate\n","      dbc: gradient of the biases of the memory gate\n","      dbo: gradient of the biases of the output gate\n","  \"\"\"\n","\n","  a_next, c_next, a_prev, c_prev, for_t, up_t, cct, out_t, xt, parameters = cache\n","\n","  # get right dimensions\n","  n_x, m = xt.shape\n","  n_a, m = a_next.shape\n","\n","  # Compute gates related derivatives\n","  dot = da_next * np.tanh(c_next) * out_t * (1 - out_t)\n","  dcct = (dc_next * up_t + out_t * (1 - np.square(np.tanh(c_next))) * up_t * da_next) * (1 - np.square(cct))\n","  dut = (dc_next * cct + out_t * (1 - np.square(np.tanh(c_next))) * cct * da_next) * up_t * (1 - up_t)\n","  dft = (dc_next * c_prev + out_t * (1 - np.square(np.tanh(c_next))) * c_prev * da_next) * for_t * (1 - for_t)\n","\n","  concat = np.concatenate((a_prev, xt), axis=0)\n","\n","  # compute parameters related derivatives\n","  dWf = np.dot(dft, concat.T)\n","  dWu = np.dot(dut, concat.T)\n","  dWc = np.dot(dcct, concat.T)\n","  dWo = np.dot(dot, concat.T)\n","  dbf = np.sum(dft, axis=1, keepdims=True)\n","  dbu = np.sum(dut, axis=1, keepdims=True)\n","  dbc = np.sum(dcct, axis=1, keepdims=True)\n","  dbo = np.sum(dot, axis=1, keepdims=True)\n","\n","  # Compute derivatives: previous hidden state, previous memory state and input\n","  Wf = parameters['Wf']\n","  Wu = parameters['Wu']\n","  Wc = parameters['Wc']\n","  Wo = parameters['Wo']\n","  da_prev = np.dot(Wf[:, :n_a].T, dft) + np.dot(Wu[:, :n_a].T, dut) + np.dot(Wc[:, :n_a].T, dcct) + np.dot(Wo[:, :n_a].T, dot)\n","  dc_prev = dc_next * for_t + out_t * (1 - np.square(np.tanh(c_next))) * for_t * da_next\n","  dxt = np.dot(Wf[:, n_a:].T, dft) + np.dot(Wu[:, n_a:].T, dut) + np.dot(Wc[:, n_a:].T, dcct) + np.dot(Wo[:, n_a:].T, dot)\n","\n","  gradients = {'dxt': dxt,\n","               'da_prev': da_prev,\n","               'dc_prev': dc_prev,\n","               'dWf': dWf,\n","               'dbf': dbf,\n","               'dWu': dWu,\n","               'dbu': dbu,\n","               'dWc': dWc,\n","               'dbc': dbc,\n","               'dWo': dWo,\n","               'dbo': dbo}\n","\n","  return gradients"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"be2OznISa51Z","executionInfo":{"status":"ok","timestamp":1603591663398,"user_tz":300,"elapsed":634,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def lstm_backward(da, caches):\n","  \"\"\"\n","  Implements the backward pass for the whole sequence \n","  of the RNN with lstm cell\n","\n","  Args:\n","    da: gradients of the hidden states\n","    cache: information from the forward pass\n","  \n","  Returns:\n","    gradients: (dict) containing:\n","      dx: gradient of inputs\n","      da0: gradient of the previous hidden state\n","      dWf: gradient of the weight matrix of the forget gate\n","      dWu: gradient of the weight matrix of the update gate\n","      dWc: gradient of the weight matrix of the memory gate\n","      dWo: gradient of the weight matrix of the output gate\n","      dbf: gradient of the baises of the fortget gate\n","      dbu: gradient of the biases of the update gate\n","      dbc: gradient of the biases of the memory gate\n","      dbo: gradient of the biases of the output gate\n","  \"\"\"\n","  caches, x = caches\n","  a1, c1, a0, c0, f1, u1, cc1, o1, x1, parameters = caches[0]\n","  \n","  # get right dimensions\n","  n_a, m, T_x = da.shape\n","  n_x, m = x1.shape\n","\n","  # initialize the gradients\n","  dx = np.zeros((n_x, m, T_x))\n","  da0 = np.zeros((n_a, m))\n","  da_prevt = np.zeros(da0.shape)\n","  dc_prevt = np.zeros(da0.shape)\n","  dWf = np.zeros((n_a, n_a + n_x))\n","  dWu = np.zeros(dWf.shape)\n","  dWc = np.zeros(dWf.shape)\n","  dWo = np.zeros(dWf.shape)\n","  dbf = np.zeros((n_a, 1))\n","  dbu = np.zeros(dbf.shape)\n","  dbc = np.zeros(dbf.shape)\n","  dbo = np.zeros(dbf.shape)\n","\n","  for timestep in reversed(range(T_x)):\n","    # Compute all gradients\n","    gradients = lstm_cell_backward(da[:, :, timestep], dc_prevt, caches[timestep])\n","\n","    # Store or add the gradient to the parameters\n","    dx[:, :, timestep] = gradients['dxt']\n","    dWf += gradients['dWf']\n","    dWu += gradients['dWu']\n","    dWc += gradients['dWc']\n","    dWo += gradients['dWo']\n","    dbf += gradients['dbf']\n","    dbu += gradients['dbu']\n","    dbc += gradients['dbc']\n","    dbo += gradients['dbo']\n","\n","  # set the first activation's gradient to the backpropagated gradient da_prev\n","  da0 = gradients['da_prev']\n","\n","  gradients = {'dx': dx,\n","               'da0': da0,\n","               'dWf': dWf,\n","               'dWu': dWu,\n","               'dWc': dWc,\n","               'dWo': dWo,\n","               'dbf': dbf,\n","               'dbu': dbu,\n","               'dbc': dbc,\n","               'dbo': dbo}\n","\n","  return gradients"],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"id":"7rYiDk2kgPiS"},"source":[""],"execution_count":null,"outputs":[]}]}