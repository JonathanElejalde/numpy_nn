{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rnn.ipynb","provenance":[],"authorship_tag":"ABX9TyNYWrdrcDFuvY1741Pz6vjd"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"akmGHfMF2eVI","executionInfo":{"status":"ok","timestamp":1603580857635,"user_tz":300,"elapsed":632,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rlOipu0d511N"},"source":["# Utility functions"]},{"cell_type":"code","metadata":{"id":"DdvXruMN52nS","executionInfo":{"status":"ok","timestamp":1603581580329,"user_tz":300,"elapsed":493,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def softmax(x):\n","  e_x = np.exp(x - np.max(x))\n","  \n","  return e_x / e_x.sum(axis=0)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"MouXgSwn3fgN","executionInfo":{"status":"ok","timestamp":1603581646290,"user_tz":300,"elapsed":714,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def rnn_cell_forward(timestep_t, a_prev, parameters):\n","  \"\"\"\n","  Implements a single forward step of an RNN cell\n","\n","  Args:\n","    timestep_t: (numpy array) input data at timestep \"t\"\n","    a_prev: (numpy array) hidden state at timestep \"t-1\"\n","    parameters: (dict) containing:\n","      Wax: (numpy array) weight matrix multiplying the input\n","      Waa: (numpy array) weight matrix multiplying the hidden state\n","      Wya: (numpy array) weight matrix relating the hiden state\n","        to the output\n","      ba: (numpy array) bias\n","      by: (numpy array) bias relating the hidden state to the output\n","\n","  Returns:\n","    a_next: (numpy array) next hidden state\n","    yt_pred: (numpy array) prediction at timestep \"t\"\n","    cache: (tuple) with values needed for the backward pass\n","      it contains (a_nex, a_prev, timestep_t, parameters)\n","  \"\"\"\n","\n","  Wax = parameters['Wax']\n","  Waa = parameters['Waa']\n","  Wya = parameters['Wya']\n","  ba = parameters['ba']\n","  by = parameters['by']\n","\n","  # Compute next activation state\n","  a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, timestep_t) + ba)\n","\n","  # Compute output of the current cell\n","  yt_pred = softmax(np.dot(Wya, a_next) + by)\n","\n","  cache = (a_next, a_prev, timestep_t, parameters)\n","\n","  return a_next, yt_pred, cache"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lMKyzSdE6PUd"},"source":["# RNN forward pass"]},{"cell_type":"code","metadata":{"id":"-lAX4hG37kl4","executionInfo":{"status":"ok","timestamp":1603582489993,"user_tz":300,"elapsed":510,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def rnn_forward(x, act0, parameters):\n","  \"\"\"\n","  Implements the forward propagation of a recurrent neural\n","  network\n","\n","  Args:\n","    x: (numpy array) input data for every timestep\n","    act0: (numpy array) initial hidden state\n","    parameters: (dict) containing:\n","      Wax: (numpy array) weight matrix multiplying the input\n","      Waa: (numpy array) weight matrix multiplying the hidden state\n","      Wya: (numpy array) weight matrix relating the hiden state\n","        to the output\n","      ba: (numpy array) bias\n","      by: (numpy array) bias relating the hidden state to the output\n","\n","  Returns:\n","    activations: (numpy array) hidden states for every timestep\n","    y_pred: (numpy array) predictions for every timestep\n","    caches: (tuple) with values needed for the backward pass\n","      it contains the list of caches and x\n","  \"\"\"\n","\n","  caches = []\n","\n","  # get dimensions from shapes of x and Wy\n","  n_x, m, T_x = x.shape\n","  n_y, n_a = parameters['Wya'].shape\n","\n","  # initialize 'a' and 'y' with zeros\n","  activations = np.zeros((n_a, m, T_x))\n","  y_pred = np.zeros((n_y, m, T_x))\n","\n","  # initialize a_next\n","  a_next = act0\n","\n","  for timestep in range(T_x):\n","    # update next hidden state, compute the prediction\n","    a_next, yt_pred, cache = rnn_cell_forward(x[:, :, timestep], a_next, parameters)\n","\n","    # save the value of the new 'next' hidden state and the prediction\n","    activations[:, :, timestep] = a_next\n","    y_pred[:, :, timestep] = yt_pred\n","    caches.append(cache)\n","\n","  caches = (caches, x)\n","\n","  return activations, y_pred, caches"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Rtp-ctb9qQY"},"source":["# Backprop in a RNN\n"]},{"cell_type":"code","metadata":{"id":"7x0m0U6iABuj","executionInfo":{"status":"ok","timestamp":1603583723196,"user_tz":300,"elapsed":519,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def rnn_cell_backward(da_next, cache):\n","  \"\"\"\n","  Implements the backward pass for a single step in the RNN-cell\n","\n","  Args:\n","    da_next: (numpy array) gradient loss with respect to next hidden state\n","    cache: (dict) contains outputs of the rnn_cell_forward()\n","\n","  Returns:\n","    gradients: (dict) containing:\n","      dx: (numpy array)\n","      da_prev: (numpy array) gradients of previous hidden state\n","      dWax: (numpy array) gradients of input to hidden weights\n","      dWaa: (numpy array) gradients of hidden to hidden weights\n","      dba: (numpy array) gradients of bias vector\n","  \"\"\"\n","\n","  a_next, a_prev, timestep_t, parameters = cache\n","\n","  Wax = parameters['Wax']\n","  Waa = parameters['Waa']\n","  Wya = parameters['Wya']\n","  ba = parameters['ba']\n","  by = parameters['by']\n","\n","  # compute the gradient of tanh with respect to a_next\n","  dtanh = (1 - a_next**2) * da_next\n","\n","  # compute the gradient of the loss with respect to Wax\n","  dxt = np.dot(Wax.T, dtanh)\n","  dWax = np.dot(dtanh, timestep_t.T)\n","\n","  # Compute the gradient with respect to Waa\n","  da_prev = np.dot(Waa.T, dtanh)\n","  dWaa = np.dot(dtanh, a_prev.T)\n","\n","  # compute the gradient with respect to b\n","  dba = np.sum(dtanh, 1, keepdims=True)\n","\n","  # store gradients\n","  gradients = {'dxt': dxt,\n","               'dWax': dWax,\n","               'da_prev': da_prev,\n","               'dWaa': dWaa,\n","               'dba': dba}\n","\n","  return gradients"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Jtts6p1Cezz","executionInfo":{"status":"ok","timestamp":1603585779888,"user_tz":300,"elapsed":618,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def rnn_backward(da, caches):\n","  \"\"\"\n","  Implements the backward pass for a RNN over an entire sequence\n","  of input data\n","\n","  Args:\n","    da: upstream gradient of all hidden states\n","    caches: (tuple) containing information from the forward pass\n","\n","  Returns:\n","    gradients: (dict) containing:\n","      dx: gradients of the input data\n","      da0: gradients of the initial hidden state\n","      dWax: gradients of the input's weights matrix\n","      dWaa: gradients of the hidden state's weights matrix\n","      dba: gradients of the bias\n","  \"\"\"\n","\n","  caches, x = caches\n","  a1, a0, x1, parameters = caches[0]\n","\n","  # get dimensions\n","  n_a, m, T_x = da.shape\n","  n_x, m = x1.shape\n","\n","  # initialize the gradients with the right sizes\n","  dx = np.zeros((n_x, m, T_x))\n","  dWax = np.zeros((n_a, n_x))\n","  dWaa = np.zeros((n_a, n_a))\n","  dba = np.zeros((n_a, 1))\n","  da0 = np.zeros((n_a, m))\n","  da_prevt = np.zeros((n_a, m))\n","\n","  for timestep in reversed(range(T_x)):\n","    # compute gradients in current timestep\n","    gradients = rnn_cell_backward(da[:, :, timestep] + da_prevt, caches[timestep])\n","\n","    # get derivatives from gradients\n","    dxt = gradients['dxt'] \n","    da_prevt = gradients['da_prev']\n","    dWaxt = gradients['dWax']\n","    dWaat = gradients['dWaa']\n","    dbat = gradients['dba']\n","\n","    # increment the global derivatives\n","    dx[:, :, timestep] = dxt\n","    dWax += dWaxt\n","    dWaa += dWaat\n","    dba += dbat\n","\n","  # set da0 to the gradient of \"a\" which has been backpropagated thorugh\n","  # all the timesteps\n","  da0 = da_prevt\n","\n","  gradients = {'dx': dx,\n","               'da0': da0,\n","               'dWax': dWax,\n","               'dWaa': dWaa,\n","               'dba': dba}\n","\n","  return gradients  \n"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"vm-af3S2KUyl"},"source":[""],"execution_count":null,"outputs":[]}]}